version: "3.9"
#Version no MCP

x-env: &default-env
  TZ: Europe/Amsterdam
  WEBUI_URL: https://team1-openwebui.valuechainhackers.xyz  # required for OAuth/SSO & some features
  ENABLE_PERSISTENT_CONFIG: "true"

services:
  openwebui:
    image: ghcr.io/open-webui/open-webui:cuda # or :main on CPU
    restart: unless-stopped
    ports: ["8080:8080"]
    extra_hosts: ["host.docker.internal:host-gateway"]
    environment:
      <<: *default-env
      # Models / connections
      ENABLE_OPENAI_API: "true"
      ENABLE_OLLAMA_API: "true"
      OLLAMA_BASE_URL: http://ollama:11434
      # Pipelines (acts like an OpenAI endpoint)
      OPENAI_API_BASE_URL: http://pipelines:9099/v1
      OPENAI_API_KEY: pipelines-pass
      # RAG / Vector DB
      VECTOR_DB: qdrant
      QDRANT_URI: http://qdrant:6333
      QDRANT_API_KEY: devkey
      ENABLE_QDRANT_MULTITENANCY_MODE: "true"
      # Web search
      ENABLE_WEB_SEARCH: "true"
      WEB_SEARCH_ENGINE: searxng
      SEARXNG_QUERY_URL: http://searxng:8080/search?q=<query>
      WEB_SEARCH_RESULT_COUNT: "5"
      # Audio (local Whisper + Kokoro TTS)
      WHISPER_MODEL: medium
      AUDIO_STT_ENGINE: "" # empty -> local Whisper
      ENABLE_IMAGE_GENERATION: "true"
      IMAGE_GENERATION_ENGINE: comfyui
      COMFYUI_BASE_URL: http://comfyui:8188
      # TTS via Kokoro FastAPI (OpenAI-compatible /v1)
      AUDIO_TTS_ENGINE: openai
      AUDIO_TTS_OPENAI_API_BASE_URL: http://kokoro:8880/v1
      AUDIO_TTS_OPENAI_API_KEY: unused
    volumes:
      - ./data:/app/backend/data
    depends_on:
      - ollama
      - pipelines
      - qdrant

  ollama:
    image: ollama/ollama:latest
    restart: unless-stopped
    ports: ["11434:11434"]
    volumes:
      - ollama:/root/.ollama

  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    restart: unless-stopped
    ports: ["9099:9099"]
    environment:
      PIPELINES_PASSWORD: pipelines-pass
    volumes:
      - pipelines:/app/pipelines

  qdrant:
    image: qdrant/qdrant:latest
    restart: unless-stopped
    ports: ["6333:6333"]
    environment:
      QDRANT__SERVICE__API_KEY: devkey

  searxng:
    image: searxng/searxng:latest
    restart: unless-stopped
    ports: ["8081:8080"]
    environment:
      BASE_URL: http://searxng:8080/

  comfyui:
    # choose a ComfyUI image that fits your GPU/host; example Nvidia build:
    image: mmartial/comfyui-nvidia-docker:latest
    restart: unless-stopped
    ports: ["8188:8188"]

  kokoro:
    image: ghcr.io/remsky/kokoro-fastapi-cpu:v0.2.2
    restart: unless-stopped
    ports: ["8880:8880"]

  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    restart: unless-stopped
    ports: ["4000:4000"]
    command: ["--port","4000"]
    # (optional) point OPENAI_API_BASE_URL to litellm in Open WebUI if you prefer a gateway

  mcpo:
    image: ghcr.io/open-webui/mcpo:main
    container_name: mcpo
    restart: unless-stopped
    ports:
      - "8000:8000"   # -> http://10.0.8.42:8000  (Traefik target)
    # Variant A: single MCP server (example: time)
    command: ["--port","8000","--api-key","${MCPO_API_KEY:-top-secret}","--",
              "uvx","mcp-server-time","--local-timezone=Europe/Amsterdam"]
    # --- Variant B: multiple MCP servers via config (uncomment to use) ---
    # volumes:
    #   - ./mcpo.json:/app/config.json:ro
    # command: ["--config","/app/config.json","--hot-reload",
    #           "--api-key","${MCPO_API_KEY:-top-secret}"]
    # ---------------------------------------------------------------
    healthcheck:
      test: ["CMD-SHELL","curl -fsS http://localhost:8000/docs >/dev/null || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s


  langfuse:
    image: ghcr.io/langfuse/langfuse:latest
    restart: unless-stopped
    ports: ["3001:3000"]
    # Provide DATABASE_URL + NEXTAUTH_SECRET if you actually enable it

  jupyter:
    image: jupyter/base-notebook:latest
    restart: unless-stopped
    ports: ["8888:8888"]
    volumes:
      - ./notebooks:/home/jovyan/work

volumes:
  ollama:
  pipelines:
