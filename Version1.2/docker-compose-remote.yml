services:
  # Main OpenWebUI Interface
  openwebui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: team1-openwebui
    restart: on-failure:1
    ports:
      - "8080:8080"
    extra_hosts:
      - "host.docker.internal:host-gateway"
    environment:
      # Core Configuration
      WEBUI_URL: https://team1-openwebui.valuechainhackers.xyz
      WEBUI_SECRET_KEY: ${WEBUI_SECRET_KEY}
      ENABLE_SIGNUP: "true"  # Temporarily enable to create first user
      DEFAULT_USER_ROLE: "admin"
      ENABLE_PERSISTENT_CONFIG: "true"
      
      # Database Configuration
      DATABASE_URL: postgresql://openwebui:${POSTGRES_PASSWORD}@postgres:5432/openwebui
      
      # Local Model Backend (Ollama)
      ENABLE_OLLAMA_API: "true"
      OLLAMA_BASE_URL: http://ollama:11434
      
      # External Models via OpenRouter
      ENABLE_OPENAI_API: "true"
      OPENAI_API_BASE_URL: https://openrouter.ai/api/v1
      OPENAI_API_KEY: ${OPENROUTER_API_KEY}
      
      # OpenWebUI Pipelines (Plugin Framework)
      PIPELINES_URLS: "http://pipelines:9099"
      
      # RAG Configuration
      VECTOR_DB: qdrant
      QDRANT_URI: http://qdrant:6333
      QDRANT_API_KEY: ${QDRANT_API_KEY}
      ENABLE_QDRANT_MULTITENANCY_MODE: "true"
      RAG_EMBEDDING_ENGINE: "openai"
      RAG_EMBEDDING_MODEL: "text-embedding-3-small"
      ENABLE_RAG_HYBRID_SEARCH: "true"
      
      # Document Processing
      CONTENT_EXTRACTION_ENGINE: "tika"
      TIKA_SERVER_URL: http://tika:9998
      
      # Web Search Integration
      ENABLE_WEB_SEARCH: "true"
      WEB_SEARCH_ENGINE: "searxng"
      SEARXNG_QUERY_URL: http://searxng:8080/search?q=<query>
      WEB_SEARCH_RESULT_COUNT: "5"
      WEB_LOADER_CONCURRENT_REQUESTS: "10"
      
      # Voice Input (Faster-Whisper STT)
      ENABLE_SPEECH_TO_TEXT: "true"
      AUDIO_STT_ENGINE: "openai"
      AUDIO_STT_OPENAI_API_BASE_URL: http://faster-whisper:10300/v1
      AUDIO_STT_OPENAI_API_KEY: "dummy-key"
      
      # Voice Output (TTS) - DISABLED for now
      ENABLE_TEXT_TO_SPEECH: "false"
      
      # Code Execution
      ENABLE_CODE_INTERPRETER: "true"
      CODE_EXECUTION_ENGINE: "jupyter"
      CODE_EXECUTION_JUPYTER_URL: http://jupyter:8888
      CODE_EXECUTION_JUPYTER_AUTH: "token"
      CODE_EXECUTION_JUPYTER_AUTH_TOKEN: ${JUPYTER_TOKEN}
      CODE_EXECUTION_JUPYTER_TIMEOUT: "60"
      
      # Image Generation via OpenRouter/External APIs
      ENABLE_IMAGE_GENERATION: "true"
      IMAGE_GENERATION_ENGINE: "openai"
      IMAGES_OPENAI_API_BASE_URL: https://openrouter.ai/api/v1
      IMAGES_OPENAI_API_KEY: ${OPENROUTER_API_KEY}
      
      # Tools and Functions
      ENABLE_OPENAI_API_FUNCTIONS: "true"
      ENABLE_PYTHON_CODE_EXECUTION: "true"
      
      # User Experience
      ENABLE_MESSAGE_RATING: "true"
      ENABLE_COMMUNITY_SHARING: "false"
      ENABLE_ADMIN_EXPORT: "true"
      ENABLE_ADMIN_CHAT_ACCESS: "true"
      
      # Security and Privacy
      WEBUI_AUTH_TRUSTED_EMAIL_HEADER: ""
      ENABLE_LOGIN_FORM: "true"
      ENABLE_OAUTH_SIGNUP: "false"
      
      # Performance and Limits
      CHUNK_SIZE: "1600"
      CHUNK_OVERLAP: "100"
      PDF_EXTRACT_IMAGES: "false"
      
    volumes:
      - openwebui-data:/app/backend/data
    depends_on:
      - postgres
      - ollama
      - pipelines
      - qdrant
      - searxng
      - jupyter
      - faster-whisper
      - tika
      - litellm
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # LiteLLM Proxy
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: team1-litellm
    restart: on-failure:1
    ports:
      - "4000:4000"
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:-sk-1234}
      DATABASE_URL: postgresql://openwebui:${POSTGRES_PASSWORD}@postgres:5432/openwebui
      REDIS_HOST: redis
      REDIS_PORT: 6379
      OPENROUTER_API_KEY: ${OPENROUTER_API_KEY}
    volumes:
      - ./litellm/config.yaml:/app/config.yaml
    depends_on:
      - postgres
      - redis
      - ollama
    command: ["--config", "/app/config.yaml", "--port", "4000", "--num_workers", "1"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  # OpenWebUI Pipelines (Plugin Framework)
  pipelines:
    image: ghcr.io/open-webui/pipelines:main
    container_name: team1-pipelines
    restart: on-failure:1
    ports:
      - "9099:9099"
    environment:
      PIPELINES_API_KEY: ${PIPELINES_API_KEY:-0p3n-w3bu!}
      # Pre-load common pipelines
      PIPELINES_URLS: |
        https://github.com/open-webui/pipelines/blob/main/examples/filters/function_calling_filter_pipeline.py,
        https://github.com/open-webui/pipelines/blob/main/examples/filters/detoxify_filter_pipeline.py,
        https://github.com/open-webui/pipelines/blob/main/examples/filters/rate_limit_filter_pipeline.py
    volumes:
      - pipelines-data:/app/pipelines
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9099/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ClickHouse for Analytics
  clickhouse:
    image: clickhouse/clickhouse-server:23
    container_name: team1-clickhouse
    restart: on-failure:1
    ports:
      - "8123:8123"   # HTTP
      - "9000:9000"   # Native
    environment:
      - CLICKHOUSE_DEFAULT_ACCESS_MANAGEMENT=1
    volumes:
      - clickhouse-data:/var/lib/clickhouse
    healthcheck:
      test: ["CMD", "wget", "--spider", "-q", "http://localhost:8123/ping"]
      interval: 30s
      timeout: 10s
      retries: 5

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    container_name: team1-postgres
    restart: on-failure:1
    environment:
      POSTGRES_DB: openwebui
      POSTGRES_USER: openwebui
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
    volumes:
      - postgres-data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U openwebui"]
      interval: 30s
      timeout: 10s
      retries: 5

  # Ollama (Local CPU Models - Optimized)
  ollama:
    image: ollama/ollama:latest
    container_name: team1-ollama
    restart: on-failure:1
    ports:
      - "11434:11434"
    environment:
      # CPU Optimization Settings
      OLLAMA_NUM_PARALLEL: "1"                    # Limit concurrent requests for CPU
      OLLAMA_MAX_LOADED_MODELS: "2"               # Reduce memory usage
      OLLAMA_HOST: "0.0.0.0"
      OLLAMA_KEEP_ALIVE: "5m"                     # Unload models after 5min
      OLLAMA_MAX_VRAM: "0"                        # Force CPU-only mode
      OLLAMA_LLM_LIBRARY: "cpu"                   # CPU library preference
      OLLAMA_FLASH_ATTENTION: "0"                 # Disable for CPU
      OLLAMA_NOHISTORY: "1"                       # Reduce memory overhead
      # Threading and Performance
      OMP_NUM_THREADS: "4"                        # Limit OpenMP threads
      MKL_NUM_THREADS: "4"                        # Intel MKL threads
      OPENBLAS_NUM_THREADS: "4"                   # OpenBLAS threads
      VECLIB_MAXIMUM_THREADS: "4"                 # Apple vecLib threads
      # Memory Management
      MALLOC_MMAP_MAX: "65536"                    # Memory mapping optimization
      MALLOC_TRIM_THRESHOLD: "100000"             # Memory trim threshold
    volumes:
      - ollama-data:/root/.ollama
    deploy:
      resources:
        limits:
          cpus: '4.0'                              # Limit CPU cores
          memory: 8G                               # Memory limit
        reservations:
          cpus: '2.0'
          memory: 4G
    healthcheck:
      test: ["CMD", "ollama", "list"]
      interval: 60s
      timeout: 10s
      retries: 3

  # Model Auto-Puller for Ollama
  ollama-puller:
    image: ollama/ollama:latest
    container_name: team1-ollama-puller
    restart: "no"
    depends_on:
      ollama:
        condition: service_healthy
    environment:
      OLLAMA_HOST: http://ollama:11434
    entrypoint: >
      sh -c '
      echo "Pulling essential models...";
      ollama pull llama3.2:3b-instruct-q4_0 &&
      ollama pull phi3:mini &&
      ollama pull nomic-embed-text &&
      ollama pull qwen2.5:0.5b-instruct &&
      echo "Essential models pulled successfully"
      '

  # Qdrant Vector Database
  qdrant:
    image: qdrant/qdrant:latest
    container_name: team1-qdrant
    restart: on-failure:1
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      QDRANT__SERVICE__API_KEY: ${QDRANT_API_KEY}
      QDRANT__SERVICE__ENABLE_CORS: "true"
      QDRANT__SERVICE__HTTP_PORT: "6333"
      QDRANT__SERVICE__GRPC_PORT: "6334"
    volumes:
      - qdrant-data:/qdrant/storage
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/localhost/6333' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # SearxNG Meta-Search Engine
  searxng:
    image: searxng/searxng:latest
    container_name: team1-searxng
    restart: on-failure:1
    ports:
      - "8081:8080"
    volumes:
      - ./searxng:/etc/searxng:rw
    environment:
      SEARXNG_BASE_URL: http://searxng:8080/
      SEARXNG_SECRET: ${SEARXNG_SECRET}
    cap_drop:
      - ALL
    cap_add:
      - CHOWN
      - SETGID
      - SETUID
      - DAC_OVERRIDE
    healthcheck:
      test: ["CMD", "wget", "--no-verbose", "--tries=1", "--spider", "http://localhost:8080/"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Faster-Whisper STT Service (LinuxServer)
  faster-whisper:
    image: lscr.io/linuxserver/faster-whisper:latest
    container_name: team1-faster-whisper
    restart: on-failure:1
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Europe/Amsterdam
      - WHISPER_MODEL=base-int8
      - WHISPER_BEAM=1
      - WHISPER_LANG=en
    volumes:
      - faster-whisper-data:/config
    ports:
      - "10300:10300"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:10300/v1/models || exit 0"]
      interval: 30s
      timeout: 10s
      retries: 10
      start_period: 60s

  # Jupyter for Code Execution
  jupyter:
    image: jupyter/datascience-notebook:latest
    container_name: team1-jupyter
    restart: on-failure:1
    ports:
      - "8888:8888"
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: ${JUPYTER_TOKEN}
      NB_UID: 1000
      NB_GID: 100
      CHOWN_HOME: "yes"
      CHOWN_HOME_OPTS: "-R"
    volumes:
      - jupyter-data:/home/jovyan/work
    command: >
      bash -c "
      pip install --quiet --no-cache-dir openai anthropic langchain qdrant-client plotly seaborn requests beautifulsoup4 pandas numpy matplotlib scikit-learn && \
      start-notebook.sh --NotebookApp.token='${JUPYTER_TOKEN}' \
      --NotebookApp.password='' --NotebookApp.allow_origin='*' \
      --NotebookApp.base_url=/ --NotebookApp.port=8888 \
      --NotebookApp.ip=0.0.0.0 --NotebookApp.allow_root=True
      "
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/api"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Apache Tika for Document Processing
  tika:
    image: apache/tika:latest
    container_name: team1-tika
    restart: on-failure:1
    ports:
      - "9998:9998"
    healthcheck:
      test: ["CMD-SHELL", "timeout 5 bash -c 'cat < /dev/null > /dev/tcp/localhost/9998' || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # MCPO for MCP Tools Integration
  mcpo:
    image: ghcr.io/open-webui/mcpo:main
    container_name: team1-mcpo
    restart: on-failure:1
    ports:
      - "8000:8000"
    environment:
      MCPO_API_KEY: ${MCPO_API_KEY}
    command: 
      - "--port"
      - "8000"
      - "--api-key"
      - "${MCPO_API_KEY}"
      - "--"
      - "uvx"
      - "mcp-server-time"
      - "--local-timezone=Europe/Amsterdam"
    healthcheck:
      test: ["CMD-SHELL", "curl -fsS http://localhost:8000/docs >/dev/null || exit 1"]
      interval: 10s
      timeout: 3s
      retries: 10
      start_period: 5s

  # Neo4j Graph Database  
  neo4j:
    image: neo4j:5-community
    container_name: team1-neo4j
    restart: on-failure:1
    ports:
      - "7474:7474"   # HTTP
      - "7687:7687"   # Bolt
    environment:
      NEO4J_AUTH: neo4j/${NEO4J_PASSWORD}
      NEO4J_PLUGINS: '["apoc", "graph-data-science"]'
      NEO4J_apoc_export_file_enabled: "true"
      NEO4J_apoc_import_file_enabled: "true"
      NEO4J_apoc_import_file_use__neo4j__config: "true"
      NEO4J_ACCEPT_LICENSE_AGREEMENT: "yes"
      # Memory settings for CPU server
      NEO4J_dbms_memory_heap_initial__size: "1G"
      NEO4J_dbms_memory_heap_max__size: "2G"
      NEO4J_dbms_memory_pagecache_size: "1G"
    volumes:
      - neo4j-data:/data
      - neo4j-logs:/logs
      - neo4j-import:/var/lib/neo4j/import
      - neo4j-plugins:/plugins
    deploy:
      resources:
        limits:
          memory: 3G
        reservations:
          memory: 1G
    healthcheck:
      test: ["CMD", "cypher-shell", "--username", "neo4j", "--password", "${NEO4J_PASSWORD}", "RETURN 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Redis for Session Management and Caching
  redis:
    image: redis:7-alpine
    container_name: team1-redis
    restart: on-failure:1
    ports:
      - "6379:6379"
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 30s
      timeout: 10s
      retries: 3

  # Watchtower for Automatic Updates
  watchtower:
    image: containrrr/watchtower:latest
    container_name: team1-watchtower
    restart: on-failure:1
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
    environment:
      WATCHTOWER_CLEANUP: "true"
      WATCHTOWER_SCHEDULE: "0 0 4 * * *"  # 4 AM daily
      WATCHTOWER_INCLUDE_STOPPED: "true"
      WATCHTOWER_REVIVE_STOPPED: "false"

volumes:
  openwebui-data:
    name: team1-openwebui-data
  postgres-data:
    name: team1-postgres-data  
  pipelines-data:
    name: team1-pipelines-data
  ollama-data:
    name: team1-ollama-data
  qdrant-data:
    name: team1-qdrant-data
  faster-whisper-data:
    name: team1-faster-whisper-data
  jupyter-data:
    name: team1-jupyter-data
  redis-data:
    name: team1-redis-data
  neo4j-data:
    name: team1-neo4j-data
  neo4j-logs:
    name: team1-neo4j-logs
  neo4j-import:
    name: team1-neo4j-import
  neo4j-plugins:
    name: team1-neo4j-plugins
  clickhouse-data:
    name: team1-clickhouse-data
